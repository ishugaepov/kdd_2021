\chapter{Pre-trained Language Model based Ranking in Baidu Search}

\textbf{Reference:}~\url{https://arxiv.org/abs/2105.11108}

\textbf{Keywords:} transformers, ranking, language models

\section*{Какую задачу решают авторы?}

Ранее мы уже говорили\footnote{\url{https://vk.com/papersreaders?w=wall-154085965_402}} про использование предобученных языковых моделей (\textit{pre-trained language models - PLM}) в информационном поиске для получения документов-кандидатов (\textit{retrieval}).
Сегодня мы рассмотрим пример использования PLM непосредственно на этапе ранжирования документов для построения итоговой выдачи.

Так же как и в случае с извлечением кандидатов, на этапе ранжирования PLM применить довольно сложно в виду ограничений на время работы.
Авторы предлагают модификацию ERNIE, которая позволяет эффективно оценить релевантность документа по запросу. \\

В отличие от большинства предыдущих работ, которые используют PLM для ранжирования, в данной работе используют не только заголовок документа, но так же и само содержание, чтобы еще лучше оценивать релевантность.

\section*{Как решают?}

Так как документы довольно часто бывают большими, авторы перед ранжированием предлагают извлекать из документа \textit{query-aware-summary} --- краткую выдержку из документа с учетом поискового запроса --- и использовать ее в качестве контента при оценке релевантности.

Кроме того, авторы предлагают предобучать модель на поисковых логах таким образом, чтобы модель училась отличать хорошие клики от плохих, так как поисковые логи содержат довольно шумные данные.

И наконец, в работе показан способ дообучения модели, чтобы ее предсказания релевантности были как можно более согласованы с классическими экспертными оценками \textit{bad, fair, good, excellent, perfect}. \\

Рассмотрим сначала как именно строится выдержка документа, затем как выглядит модель, и наконец как она обучается.

\subsection*{Query-aware-summary}

Выдержка (summary) в данном случае будет представлять из себя некоторый набор предложений, извлеченных из документа.
Причем количество извлеченных предложений - это гиперпараметр алгоритма.

Рассмотрим в общих чертах как выполняется построение \textit{query-aware-summary}.
\begin{enumerate}
    \item Для каждого слова из запроса считается его важность, с помощью словаря важности слов (авторы не уточняют как именно они построили словарь важности)
    \item Для каждого предложения в документе считается величина того, насколько данное предложение подходит в качестве summary.
        Данная величина для предложения есть сумма важностей слов, которые встречаются как в запросе так и в предложении.
        
        То есть, чем больше в предложении важных слов из запроса, тем больше оно подходит в качестве фрагмента выдержки документа.
    \item Выбирается предложение с наибольшим посчитанным скором.
        Выбранное предложение удаляется из текста, и уменьшается важность слов, которые содержатся как в запросе так и в предложении.
    \item Шаги 2-3 можно повторять ровно столько раз, сколько нам нужно предложений в качестве выдержки.
\end{enumerate}

\subsection*{Pyramid-ERNIE}

Общая схема модели представлена на рисунке~\ref{fig:pyramid_ernie}

\begin{figure}[ht]
  \centering
  \includegraphics[width=1.0\linewidth]{images/pyramid_ernie.png}
  \caption{\footnotesize{Illustration of the Pyramid-ERNIE model}}
  \label{fig:pyramid_ernie}
\end{figure}

Модель состоит из двух уровней:
\begin{itemize}
    \item \textbf{Representation Module} --- в рамках данного уровня независимо друг от друга стоются представления для пары запрос-заголовок, и для выдержки документа
    \item \textbf{Interaction Module} --- данный уровень моделирует взаимодействие между построенными ранее представлениями для предсказания итогового значения релевантности документа по запросу
\end{itemize}

\subsection*{Pre-training}

Прежде чем начать использовать PLM для ранжирования документов, нужно привнести в модель информацию о данной задаче.
Для этого выполняется предобучение модели на поисковых логах.

Однако, поисковые логи довольно шумные, поэтому нельзя рассматривать все результаты, по которым были клики, в качестве действительно релевантных документов по запросу.
Для получения истинных значений релевантности, как правило, прибегают к разметке экспертами.

Но обучение больших языковых моделей требует огромного количества обучающих примеров, и датасет такого размера нет возможности получить с помощью ручной разметки. \\

Для решения описанной выше проблемы, авторы предлагают научиться \textit{автоматически} размечать пары запрос-документ так же как это делают эксперты - по шкале \textit{bad, fair, good, excellent, perfect}, что позволяет получить датасет сколь угодно большого размера.

Для этого авторы обучают небольшую модель (Decision Tree), которая умеет предсказывать релевантность на основе пост-кликовых признаков ().
Данную модель обучают на датасете из 75к примеров, подготовленных экспертами. \\

Таким образом, в распоряжении авторов есть автоматически размеченный датасет из почти 3B пар запрос-документ.
Так как релевантность имеет значение от 0 до 4, то для обучения основной модели авторы используют pairwise loss (для конкретного запроса):
\begin{equation*}
    \sum\limits_{r(d_i) < r(d_j)} \max (0, f(q, d_i) - f(q, d_j) + \tau),
\end{equation*}
где $r(d_i)$ - релевантность документа, $f(q, d_i)$ - предсказанное значение релевантности документа по запросу, $\tau$ - margin enforced between positive and negative pairs.

\subsection*{Fine-tuning}

Для того чтобы предсказания предобученной модели можно было использовать наряду с другими факторами в итоговом ранжировании, авторы стараются сделать предсказания модели как можно более консистентными с экспертными оценками. \\

Чтобы добиться консистентности предсказаний и экспертных оценок, авторы используют дополнительный датасет из 10М пар, который был получен экспертами через crowd платформу.

При fine-tuning'e к указанному выше pairwise лоссу добавляют pointwise слагаемое, которое форсит консистентость:

\begin{align*}
    & \sum\limits_{y_i < y_j} \max (0, f(q, d_i) - f(q, d_j) + \tau) \\
    & + \lambda \left(\delta (f(q, d_i), y_i) + \delta (f(q, d_j), y_j)\right),
\end{align*}
где $\delta (f(q, d), y) = \max \left\{ (f(q, d) - \frac{y}{5})^2 - \eps , 0 \right\} $.

\section*{Мое мнение}

Очень интересный способ использования контента документа для оценки релевантности, который, по заявлению авторов статьи, позволил добиться существенного роста метрик в онлайне.

Жаль, что в статье нет информации о том, как много времени занимает применение данной модели в ранжировании.